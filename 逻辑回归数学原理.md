---
title: 逻辑回归数学原理
date: 2018-06-22 18:40:17
categories: 人工智能
mathjax: true
tags:
    - 逻辑回归
---
### 求解原理

#### Sigmoid 函数

$$
y={1\over {1+e^{-z}}}
$$

先看看这个函数样子：

![](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=33afcd8303f79052fb124f6c6d9abcaf/d009b3de9c82d158dfb4e7218a0a19d8bc3e426f.jpg)

可以看到，无论z取值如何，这个函数的结果总是在0到1之间。且以原点对称。

0和1，我们是不是联想到概率呢？概率总是在0与1之间取值。

#### 联想

假设有一堆数据。这些数据有一些特征值，还有一个分类结果。那么我们可不可以像线性回归那样通过特征值求出一个参数向量$\theta$,使模拟出来的分类的概率跟真实分类最接近呢？如下表达：
$$
h_\theta(x)={1\over {1+{e^{-\theta^TX}}}}
$$
其中，$h_\theta(x)$为每个样式计算出来的概率值，比如为80%（0.8），即为分类1的概率为80%。比如，已知某样式分类为1，$h_\theta(x)$就是出现该样本数据分类为1的概率。我们当然是希望概率越符合越好。即如果真实分类为分类1，我们希望概率值越大越好，反之，我们希望概率越小越好。如果有N个样本，那么符合每一个分类的概率为他们的联合概率。即计算累乘。
#### 公式推导

设分类为1时，概率为$h_\theta(x)$，那么分类为0时就是$1-h_\theta(x)$。那么：
$$
\begin{cases}
P(y_i=1|x_i;\theta)=h_\theta(x_i) ={1\over {1+{e^{-\theta^Tx_i}}}} \\
P(y_i=0|x_i;\theta)=1-h_\theta(x_i)=1-{1\over {1+{e^{-\theta^Tx_i}}}} \\
\end{cases}
$$
整合上面两个式子，可以设样本如此分类出现的概率为：
$$
P(y_i|x_i;\theta)=h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i}
$$
上面那个式子是要解释一下的，因为一开始我也看不懂。

因为y只能取1和0，那么试一下将1和0代入上面那个式子看看？上面那个式子并不是推导出来的，而是设出来的。我们为了可以计算y=0或者1的概率，即随意计算这两个结果的概率，而不是只能计算其中一个值，于是设出上面那个式子。显然，无论y取值是1还是0，结果都是对的。

有了上面那个式子，那么，根据联合概率公式（这个公式是机器学习中非常常见），所有样本这样出现的总概率为：
$$
P(y|x;\theta)= \prod_{i=1}^m P(y_i|x_i;\theta)=\prod_{i=1}^m h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i}
$$
取对数，得：
$$
ln(P(y|x;\theta))=ln( \prod_{i=1}^m P(y_i|x_i;\theta))=ln(\prod_{i=1}^m h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i})
$$
于是，我们可以这样求偏导：
$$
(ln(P(y|x;\theta)))'=(ln(\prod_{i=1}^m h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i}))' \\
=(\sum_{i=1}^m(y_ilnh_\theta(x_i)+(1-y_i)ln(1-h_\theta(x_i))))' \\
=\sum_{i=1}^m(y_i {1\over {h_\theta(x_i)}}-(1-y_i){1\over{1-h_\theta(x_i)}})(h_\theta(x_i))'
$$
问题来了，
$$
(h_\theta(x))'
$$


是什么？上面已知：
$$
h_\theta(x)={1\over {1+{e^{-\theta^TX}}}}
$$
所以，关键是对Sigmoid 函数求导。

#### sigmoid函数求导

$$
(h_\theta(x_i))'=({1\over {1+{e^{-\theta^Tx_i}}}})' \\
$$

设$z=\theta^Tx_i$,$y={1\over{1+e^{-z}}}$我们有：
$$
(h_\theta(x_i))' \\
=({1\over {1+{e^{-\theta^Tx_i}}}})' \\
=({1\over {1+{e^{-z}}}})'  \\
=-{1\over({1+e^{-z}})^2}(1+e^{-z})' \\
=-{1\over({1+e^{-z}})^2}(-e^{-z}) \\

={1\over{1+e^{-z}}}-{1\over({1+e^{-z}})^2} \\
=y(1-y)
$$
使用上面的变量是为了简化表达。

#### 继续推导

所以，我们继续上面的偏导推导,设$h_\theta(x_i)=g(\theta^Tx_i)$：
$$
(ln(P(y|x;\theta)))'=(ln(\prod_{i=1}^m h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i}))' \\
=(\sum_{i=1}^m(y_ilnh_\theta(x_i)+(1-y_i)ln(1-h_\theta(x_i))))' \\
=\sum_{i=1}^m(y_i {1\over {h_\theta(x_i)}}-(1-y_i){1\over{1-h_\theta(x_i)}})(h_\theta(x_i))' \\
=\sum_{i=1}^m(y_i {1\over {g(\theta^Tx_i)}}-(1-y_i){1\over{1-g(\theta^Tx_i)}})g(\theta^Tx_i)(1-g(\theta^Tx_i))(\theta^Tx_i)' \\
=\sum_{i=1}^m(y_i {1\over {g(\theta^Tx_i)}}-(1-y_i){1\over{1-g(\theta^Tx_i)}})g(\theta^Tx_i)(1-g(\theta^Tx_i))x_i^j \\
=\sum_{i=1}^m (y_i-g(\theta^Tx_i))x_i^j
$$
其中，$x_i^j$表示第i行，第j个特征值样本。

#### 梯度下降

通过上面的推导，我们已经求得了在逻辑回归中，任意一个参数$\theta_j$的迭代方式。设$\alpha$为导数迭代的步长。为了求出$\theta$向量，我们可以使用梯度下降算法。首先，我们随意初始化一个$\theta$向量，然后不断向着梯度下降的方向进行迭代，直到每一个向量值不再下降为止。所以迭代方式：
$$
\theta_j := \theta_j-\alpha{1\over m}\sum_{i=1}^m (y_i-g(\theta^Tx_i))x_i^j
$$
只要迭代的方向总是与导数正负相反，就会向下降的方向。${1\over m}$是为了消除累加影响。$\alpha$一般取值很小，以减少步长。

通过上面的方式，我们就巧妙地算出了最佳参数向量。