---
title: PCA数学原理理解
date: 2018-06-10 10:31:40
tags: PCA
categories: 人工智能
mathjax: true
---

本文讨论对PCA数学原理的理解。

原文中最难理解的是：

1. 协方差矩阵的特征向量为什么是原始数据的降维方向？
2. 协方差矩阵为什么可以并且要对角化？

这两个问题其实是一个问题。

我们知道，协方差矩阵是一个对称矩阵，即：
$$
CC^T=C^TC
$$
线性代数中，任何一个实对称矩阵，都有这样的特征：

> 1. 它的特征向量一定正交。
>
> 2. 特征向量单位化后组成的矩阵一定可以将原对称矩阵对角化。即：
>    $$
>    E^T C E=\Lambda
>    $$
>

请理解我们要解决的根本问题：我们的目标是求得一个矩阵P，使Y=PX，最终结果是Y尽可能保留原数据的特征。那么，在数学上说，是不是我们要求得一个矩阵P，使得变换后的数据矩阵Y的协方差矩阵D不同维度的协方差为0？只要不同维度的协方差为0，那么不同的维度是不是相互独立，互不影响了呢？这样一来，重复的维度又不大重要的维度都被去掉了！

那么，上面的协方差为0，不就是一个对角矩阵$\Lambda$嘛！对角矩阵除了了对角线的元素之外，其他元素都是0. 正好满足了我们要求的目标。

上面我们知道，原始数据X虽然不对称，但它的协方差矩阵C是对称的，设C的特征向量矩阵为E，于是：
$$
E^TCE=\Lambda
$$
我们还需要推导另一个关系,设D为Y的协方差矩阵，有：
$$
D={1 \over m}YY^T={1 \over m}(PX)(PX)^T={1 \over m}PXX^TP^T=P({1 \over m}XX^T)P^T=PCP^T
$$
我们的目标是令D成为一个对角矩阵。于是：
$$
D=\Lambda=PCP^T=E^TCE
$$
$E^T$就是我们最终要求的结果。即：$P=E^T$。

所以：
$$
Y=PX=E^TX
$$
我们知道，X是已知的，于是我们可以求得对应的协方差矩阵C，通过C可以求特征向量矩阵$E^T$，从而求得对应的对角矩阵$\Lambda$。而事实上，$\Lambda$就是特征向量矩阵对应的对角为特征值的对角矩阵。我们按照特征值大小排序就是可以去掉特征值特别小的特征向量了。所以P是我们去掉一些特征后的最终结果。