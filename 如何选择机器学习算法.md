---
title: 如何选择机器学习算法
date: 2018-06-25 21:40:49  
categories: 人工智能
tags:
    - 机器学习
---

一个典型的问题是“我应该使用哪种算法？”

问题的答案取决于许多因素，其中包括：

数据的大小，质量和性质。

可用的计算时间。

任务的紧迫性。

你想要对数据做什么。

机器学习算法备忘单

![](http://pava02hb7.bkt.clouddn.com/1.jpeg)

机器学习算法映射

![](http://pava02hb7.bkt.clouddn.com/2.jpeg)

谁将很快占领人工智能的世界!!

![](http://pava02hb7.bkt.clouddn.com/3.jpeg)

机器学习算法的类型

监督学习

监督学习算法基于一组示例进行预测。例如，历史销售额可用于估算未来价格。通过监督学习，您可以获得一个输入变量，该输入变量由标记的训练数据和一个期望的输出变量组成 您使用算法分析训练数据以了解将输入映射到输出的函数。这个推断的功能通过从训练数据中推广来预测未见情况下的结果，映射新的未知示例。

分类：当数据被用于预测分类变量时，监督学习也被称为分类。将标签或指示器（狗或猫）分配给图像时就是这种情况。当只有两个标签时，这称为二进制分类。当有两个以上的类别时，这些问题被称为多类分类。

回归：在预测连续值时，问题成为回归问题。

预测：这是根据过去和现在的数据对未来进行预测的过程。它最常用于分析趋势。一个常见的例子可能是根据当年和前几年的销售情况估算下一年的销售额。

半监督学习

监督式学习带来的挑战是标签数据可能很昂贵而且耗时。如果标签有限，可以使用未标记的示例来增强监督式学习。因为在这种情况下机器没有完全监督，所以我们说机器是半监督的。使用半监督学习，您可以使用带有少量标签数据的未标记示例来提高学习的准确性。

无监督学习

在进行无监督学习时，机器会显示完全未标记的数据。要求发现数据基础的内在模式，如聚类结构，低维流形或稀疏树和图。

聚类：将一组数据示例分组，以便一个组（或一个聚类）中的示例与其他组中的示例更相似（根据某些标准）。这通常用于将整个数据集分成几个组。可以在每个组中进行分析以帮助用户找到固有模式。

降维：减少考虑中的变量数量。在许多应用中，原始数据具有非常高的维度特征，并且一些特征对于任务是多余的或不相关的。减少维度有助于找到真实的，潜在的关系。

强化学习

强化学习根据环境反馈分析和优化代理的行为。机器尝试不同的场景来发现哪些行为产生最大的回报，而不是被告知要采取哪些行动。反复试验和延迟奖励将强化学习与其他技术区分开来。

选择算法时的注意事项

选择算法时，请始终考虑以下方面：准确性，培训时间和易用性。许多用户首先提出了准确性，而初学者倾向于关注他们最熟悉的算法。

当呈现数据集时，首先要考虑的是如何获得结果，无论结果如何。初学者倾向于选择易于实现并能够快速获得结果的算法。这很好，只要它只是这个过程的第一步。一旦您获得了一些结果并熟悉数据，您可能会花更多时间使用更复杂的算法来加强对数据的理解，从而进一步改进结果。

即使在这个阶段，最好的算法可能不是那些已经达到最高报告准确率的方法，因为算法通常需要仔细调整和广泛的培训以获得其最佳可实现的性能。

何时使用特定的算法

仔细观察个别算法可以帮助您了解它们提供的内容以及它们的使用方式。这些描述提供了更多细节，并提供了何时使用特定算法的附加提示，与备忘单一致。

线性回归和Logistic回归

![](http://pava02hb7.bkt.clouddn.com/4.jpeg)

线性回归

![](http://pava02hb7.bkt.clouddn.com/5.jpeg)

逻辑回归

线性回归是模拟连续因变量yy与一个或多个预测因子XX之间关系的一种方法。

在逻辑回归中，我们使用不同的假设类来尝试预测给定示例属于“1”类的概率与其属于“-1”类的概率。

线性SVM和内核SVM

内核技巧用于将非线性可分的函数映射为更高维的线性可分的函数。支持向量机（SVM）训练算法找到由超平面的法向量ww和偏置量bb表示的分类器。这个超平面（边界）尽可能宽地分隔不同的类。该问题可以转化为约束优化问题：

minimizewsubject to||w||yi(wTXib)≥1,i=1,…,n.minimizew||w||subject toyi(wTXib)≥1,i=1,…,n.

支持向量机（SVM）训练算法找到由法向量表示的分类器和超平面的偏差。这个超平面（边界）尽可能宽地分隔不同的类。该问题可以转化为约束优化问题：

![](http://pava02hb7.bkt.clouddn.com/6.jpeg)

内核技巧用于将非线性可分的函数映射为更高维的线性可分的函数

当类不是线性可分的时候，可以使用内核技巧将非线性可分的空间映射到更高维的线性可分的空间。

当大多数因变量是数字时，logistic回归和SVM应该是分类的第一次尝试。这些模型易于实现，参数调整容易，性能也相当不错。所以这些模型适合初学者。

Trees and ensemble trees

![](http://pava02hb7.bkt.clouddn.com/7.jpeg)

预测模型的决策树

决策树，随机森林和梯度提升都是基于决策树的算法。决策树有许多变体，但它们都做同样的事情 - 将特征空间细分成大多数标签相同的区域。决策树很容易理解和实施。但是，当我们耗尽树枝并且深入树木时，它们倾向于过度拟合数据。随机Forrest和渐变提升是两种使用树算法实现良好精确度以及克服过度拟合问题的流行方法。

神经网络和深度学习

![](http://pava02hb7.bkt.clouddn.com/8.jpeg)

卷积神经网络架构

神经网络在20世纪80年代中期由于其并行和分布式处理能力而兴旺发达。但是在这个领域的研究受到广泛用于优化神经网络参数的反向传播训练算法的无效性的阻碍。支持向量机（SVM）和其他更简单的模型，通过求解凸优化问题可以很容易地进行训练，逐渐取代机器学习中的神经网络。

近年来，诸如无监督预训练和分层贪婪训练等新的和改进的训练技术已经导致对神经网络重新兴趣。日益强大的计算能力，例如图形处理单元（GPU）和大规模并行处理（MPP），也刺激了神经网络的复兴。神经网络的复兴研究已经引发了具有数千层的模型的发明。

![](http://pava02hb7.bkt.clouddn.com/9.jpeg)

SAS Visual Analytics中的神经网络

换句话说，浅层神经网络已演变成深度学习神经网络。深度神经网络对于监督学习非常成功。当用于语音和图像识别时，深度学习的表现与人类一样好，甚至更好。应用于无监督学习任务（如特征提取），深度学习还可从原始图像或语音中提取特征，而人工干预则更少。

神经网络由三部分组成：输入层，隐藏层和输出层。训练样本定义了输入和输出层。当输出层是一个分类变量时，那么神经网络就是解决分类问题的一种方法。当输出层是一个连续变量时，网络可以用来做回归。当输出层与输入层相同时，网络可用于提取内在特征。隐藏层的数量定义了模型的复杂性和建模能力。

深度学习

K均值/ k模式，GMM（高斯混合模型）聚类

![](http://pava02hb7.bkt.clouddn.com/10.jpeg)

K均值聚类

![](http://pava02hb7.bkt.clouddn.com/11.jpeg)

高斯混合模型

Kmeans/k-modes, GMM聚类旨在将n个观测分为k个聚类。K-means定义硬分配：样本将被且仅被关联到一个群集。然而，GMM为每个样品定义一个软分配。每个样本都有与每个群集关联的概率。当给定聚类数目k时，两种算法都足够简单快速地进行聚类。

DBSCAN

![](http://pava02hb7.bkt.clouddn.com/12.jpeg)

当未给定群集k的数量时，可以通过密度扩散将样本连接起来使用DBSCAN(基于密度的空间聚类)。

分层聚类

![](http://pava02hb7.bkt.clouddn.com/13.jpeg)

可以使用树结构(dendrogram)可视化分层分区。它不需要作为输入的集群数量，并且可以在不同级别的粒度上查看分区(i.e., can refine/coarsen clusters)使用不同的K。

PCA，SVD和LDA

我们通常不希望直接将大量特征提供给机器学习算法，因为某些特征可能不相关，或者“内在”维度可能小于特征的数量。主成分分析（PCA），奇异值分解（SVD）和 潜在狄利克雷分配（LDA）均可用于降维。

PCA是一种无监督聚类方法，它将原始数据空间映射到较低维空间，同时保留尽可能多的信息。PCA基本上找到最能保留数据方差的子空间，子空间由数据协方差矩阵的主要特征向量定义。

SVD与PCA有关，因为中心数据矩阵（特征对样本）的SVD提供定义与PCA发现的相同子空间的主导左奇异向量。然而，SVD是一种更通用的技术，因为它也可以做PCA不能做的事情。例如，用户对电影矩阵的SVD能够提取可以在推荐系统中使用的用户简档和电影简档。另外，在自然语言处理（NLP）中，SVD还被广泛用作主题建模工具，称为潜在语义分析。

NLP中的相关技术是潜在Dirichlet分配（LDA）。LDA是概率性主题模型，它以与高斯混合模型（GMM）将连续数据分解为高斯密度相似的方式将文档分解为主题。与GMM不同的是，LDA对离散数据（文档中的词）进行建模，并且它约束根据Dirichlet分布该主题是先验分布的。